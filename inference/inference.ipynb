{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements for Qwen_GUIModel\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLProcessor, Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers.generation import GenerationConfig\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import os\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pdb\n",
    "import tempfile\n",
    "import ast\n",
    "\n",
    "# Draw point on image\n",
    "from PIL import ImageDraw\n",
    "import numpy as np\n",
    "    \n",
    "from transformers.models.qwen2_vl.image_processing_qwen2_vl_fast import smart_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def convert_pil_image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode()\n",
    "\n",
    "def image_to_temp_filename(image):\n",
    "    temp_file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    image.save(temp_file.name)\n",
    "    print(f\"Image saved to temporary file: {temp_file.name}\")\n",
    "    return temp_file.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZonUIModel():\n",
    "    def load_model(self, model_name_or_path=\"zonghanHZH/ZonUI-3B\"):\n",
    "        self.min_pixels = 256*28*28\n",
    "        self.max_pixels = 1280*28*28\n",
    "        self.device = \"cuda\"\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_name_or_path, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "        ).eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "        # Setting default generation config\n",
    "        self.generation_config = GenerationConfig.from_pretrained(model_name_or_path, trust_remote_code=True).to_dict()\n",
    "        self.set_generation_config(\n",
    "            max_length=4096,\n",
    "            do_sample=False,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    def set_generation_config(self, **kwargs):\n",
    "        self.generation_config.update(**kwargs)\n",
    "        self.model.generation_config = GenerationConfig(**self.generation_config)\n",
    "\n",
    "    def ground_only_positive(self, instruction, image):\n",
    "        if isinstance(image, str):\n",
    "            image_path = image\n",
    "            assert os.path.exists(image_path) and os.path.isfile(image_path), \"Invalid input image path.\"\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        assert isinstance(image, Image.Image), \"Invalid input image.\"\n",
    "        \n",
    "        # Calculate the real image size sent into the model\n",
    "        resized_height, resized_width = smart_resize(\n",
    "            image.height,\n",
    "            image.width,\n",
    "            factor=self.processor.image_processor.patch_size * self.processor.image_processor.merge_size,\n",
    "            min_pixels=self.processor.image_processor.min_pixels,\n",
    "            max_pixels=1280*28*28,\n",
    "        )\n",
    "        resized_image = image.resize((resized_width, resized_height))\n",
    "        \n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Based on the screenshot of the page, I give a text description and you give its corresponding location. The coordinate represents a clickable location [x, y] for an element.\"},\n",
    "                    {\"type\": \"image\", \"image\": image_path, \"min_pixels\": self.min_pixels, \"max_pixels\": self.max_pixels},\n",
    "                    {\"type\": \"text\", \"text\": instruction}\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True,\n",
    "        )\n",
    "        \n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=[resized_image],\n",
    "            return_tensors=\"pt\",\n",
    "            training=False\n",
    "        ).to(self.device)\n",
    "\n",
    "        generated_ids = self.model.generate(**inputs)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        response = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0].strip()\n",
    "        # print(response)\n",
    "\n",
    "        result_dict = {\n",
    "            \"result\": \"positive\",\n",
    "            \"format\": \"x1y1x2y2\",\n",
    "            \"raw_response\": response,\n",
    "            \"bbox\": None,\n",
    "            \"point\": None\n",
    "        }\n",
    "\n",
    "        # Parse action and visualize\n",
    "        try:\n",
    "            coordinates = ast.literal_eval(response)\n",
    "            if len(coordinates) == 2:\n",
    "                point_x, point_y = coordinates\n",
    "            elif len(coordinates) == 4:\n",
    "                x1, y1, x2, y2 = coordinates\n",
    "                point_x = (x1 + x2) / 2\n",
    "                point_y = (y1 + y2) / 2\n",
    "            else:\n",
    "                raise ValueError(\"Wrong output format\")\n",
    "            result_dict[\"point\"] = [point_x / resized_width, point_y / resized_height]  # Normalize predicted coordinates\n",
    "        except (IndexError, KeyError, TypeError, ValueError) as e:\n",
    "            pass\n",
    "        \n",
    "        return result_dict\n",
    "\n",
    "    def ground_allow_negative(self, instruction, image):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "def draw_point_bbox_gui(image_path, point=None, bbox=None, radius=5, line=3):\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    width, height = image.size\n",
    "    \n",
    "    if point is not None:\n",
    "        x, y = point[0], point[1]\n",
    "        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='green', outline='green')\n",
    "    if bbox is not None:\n",
    "        x1, y1, x2, y2 = bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "        draw.rectangle([x1, y1, x2, y2], outline='red', width=line)\n",
    "\n",
    "    image_draw = np.array(image)\n",
    "    return image_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e21c4c23cb41afb722c1a22ae79419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ZonUIModel:\n",
      "----------------------------------------\n",
      "Input:\n",
      "Get OmniFocus 4\n",
      "Output:\n",
      "{'result': 'positive', 'format': 'x1y1x2y2', 'raw_response': '[682, 315]', 'bbox': None, 'point': [0.5664451827242525, 0.3879310344827586]}\n",
      "Result saved to output.png\n"
     ]
    }
   ],
   "source": [
    "# Example usage with Qwen_GUIModel\n",
    "model_gui = ZonUIModel()\n",
    "model_gui.load_model(\"zonghanHZH/ZonUI-3B\")\n",
    "\n",
    "# Test with the same image and instruction\n",
    "img_url = './app_store.png'\n",
    "instruction = 'Get OmniFocus 4'\n",
    "\n",
    "print(\"Using ZonUIModel:\")\n",
    "print(\"--\" * 20)\n",
    "print(\"Input:\")\n",
    "print(instruction)\n",
    "\n",
    "result = model_gui.ground_only_positive(instruction, img_url)\n",
    "print(\"Output:\")\n",
    "print(result)\n",
    "\n",
    "# Visualize the result if point is found\n",
    "if result[\"point\"] is not None:\n",
    "    # Convert normalized coordinates back to image coordinates\n",
    "    image = Image.open(img_url)\n",
    "    width, height = image.size\n",
    "    point_x = result[\"point\"][0] * width\n",
    "    point_y = result[\"point\"][1] * height\n",
    "    \n",
    "    img_array = draw_point_bbox_gui(img_url, [point_x, point_y], bbox=None, radius=5, line=3)\n",
    "    img = Image.fromarray(img_array)\n",
    "    img.save(\"./output.png\")\n",
    "    print(f\"Result saved to output.png\")\n",
    "else:\n",
    "    print(\"No valid point found in the response\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
